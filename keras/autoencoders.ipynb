{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Utility-Functions\" data-toc-modified-id=\"Utility-Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Utility Functions</a></div><div class=\"lev1 toc-item\"><a href=\"#MNIST\" data-toc-modified-id=\"MNIST-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>MNIST</a></div><div class=\"lev2 toc-item\"><a href=\"#Deep-Autoencoder\" data-toc-modified-id=\"Deep-Autoencoder-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Deep Autoencoder</a></div><div class=\"lev2 toc-item\"><a href=\"#Shallow-Autoencoder\" data-toc-modified-id=\"Shallow-Autoencoder-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Shallow Autoencoder</a></div><div class=\"lev1 toc-item\"><a href=\"#Denoising-Autoencoder\" data-toc-modified-id=\"Denoising-Autoencoder-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Denoising Autoencoder</a></div><div class=\"lev1 toc-item\"><a href=\"#Sparse-Autoencoders\" data-toc-modified-id=\"Sparse-Autoencoders-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sparse Autoencoders</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Author: Roy Wilds\n",
    "Created: 2020-12-12\n",
    "Description: Combining various sources (see credits below) to provide a complete example of using an Autoencoder with Keras (with Tensorflow) that covers the \"gotchas\" that I found challenging and provide some insight into what the model is doing.\n",
    "\n",
    "Updates:\n",
    "2021-01-03 - Typos fixed and additional descriptions added to make things clearer.\n",
    "```\n",
    "\n",
    "Credits:\n",
    "- Relies heavily on the great work from https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%203%20-%20Autoencoders.ipynb \n",
    "- Incorporates material from E2EML Course 312: https://end-to-end-machine-learning.teachable.com/courses/enrolled/612528\n",
    "\n",
    "Layout of this notebook is\n",
    "- Import what's needed and then define some handy functions.\n",
    "- Create our dataset\n",
    "- Learn a simple 5 layer autoencoder model and then understand what it's doing.\n",
    "- Learn a sparse 3 layer autoencoder model (just 1 hidden layer). Compare with a non-sparse model of the same dimensions to see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.datasets import mnist\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "Small modifications from original: https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%203%20-%20Autoencoders.ipynb\n",
    "\n",
    "Keep in mind that these functions rely on having access to some global variables (e.g. `x_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autoencoder_outputs(autoencoder, dims, n=5):\n",
    "    decoded_imgs = autoencoder.predict(x_test)\n",
    "    plt.figure(figsize=(10, 4.5))\n",
    "    for i in range(n):\n",
    "        # plot original image\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_test[i].reshape(*dims))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == n/2:\n",
    "            ax.set_title('Original Images')\n",
    "\n",
    "        # plot reconstruction \n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(*dims))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == n/2:\n",
    "            ax.set_title('Reconstructed Images')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, historydf.values.max()))\n",
    "    plt.title('Loss: %.3f' % history.history['loss'][-1])\n",
    "    \n",
    "def plot_compare_histories(history_list, name_list, plot_accuracy=True):\n",
    "    dflist = []\n",
    "    min_epoch = len(history_list[0].epoch)\n",
    "    losses = []\n",
    "    for history in history_list:\n",
    "        h = {key: val for key, val in history.history.items() if not key.startswith('val_')}\n",
    "        dflist.append(pd.DataFrame(h, index=history.epoch))\n",
    "        min_epoch = min(min_epoch, len(history.epoch))\n",
    "        losses.append(h['loss'][-1])\n",
    "\n",
    "    historydf = pd.concat(dflist, axis=1)\n",
    "\n",
    "    metrics = dflist[0].columns\n",
    "    idx = pd.MultiIndex.from_product([name_list, metrics], names=['model', 'metric'])\n",
    "    historydf.columns = idx\n",
    "    \n",
    "    plt.figure(figsize=(6, 8))\n",
    "\n",
    "    ax = plt.subplot(211)\n",
    "    historydf.xs('loss', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "    plt.title(\"Training Loss: \" + ' vs '.join([str(round(x, 3)) for x in losses]))\n",
    "    \n",
    "    if plot_accuracy:\n",
    "        ax = plt.subplot(212)\n",
    "        historydf.xs('acc', axis=1, level='metric').plot(ylim=(0,1), ax=ax)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "    \n",
    "    plt.xlim(0, min_epoch-1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Sticking with Arden Dertat's article/code, we'll use the MNIST dataset. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize!\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "# Reshape so that we have a flat vector for each image\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Autoencoder Structure\n",
    "Define our layer sizes. We have:\n",
    "- Input Layer (layer size is determined by image size)\n",
    "- Encoder Hidden Layer (first hidden layer)\n",
    "- Code layer (our middle layer that will be our \"coded\" representation)\n",
    "- Decoder Hidden Layer (same size as Encoder Hidden Layer)\n",
    "- Output Layer (same size as Input Layer)\n",
    "\n",
    "To start, we're just going to train a vanilla autoencoder. This will learn whatever weights the optimizer happens to settle on after training. \n",
    "\n",
    "In later sections we'll see how to constrain the weights learned based on useful properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784  # 784 = 28*28 (images are 28 by 28 pixels... grayscaled)\n",
    "hidden_size = 128  # Dimension of our single hidden layer. \n",
    "code_size = 32  # Dimension of our \"coded\" or compressed representation. 32 is much less than the 784 input dimension.\n",
    "\n",
    "# This is how you construct your network in Keras. Chaining the layers by referencing the previous layer.\n",
    "# Really damn cool and simple.\n",
    "input_img = Input(shape=(input_size,))\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(input_img)\n",
    "code = Dense(code_size, activation='relu')(hidden_1)\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(code)\n",
    "output_img = Dense(input_size, activation='sigmoid')(hidden_2)\n",
    "\n",
    "# With our layers defined, then the full ANN model is created by specifying the input and output layers.\n",
    "autoencoder = Model(input_img, output_img)\n",
    "# Define our optimizer and loss function\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# And train it. \n",
    "# Automatically uses multiple cores (if you have them). Can also use GPUs if you've got them.\n",
    "autoencoder.fit(x_train, x_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "## Accuracy\n",
    "With our model trained, let's compare some inputs to outputs using our handy plotting functions from earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the learned model, the image size (must be same used for the input layer size reshaped to be 2-d), \n",
    "# and the number of images to show (default is 5)\n",
    "# Note that the held out test set x_test is used.\n",
    "plot_autoencoder_outputs(autoencoder, (28, 28), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good considering that the inputs are 784 dimensional vectors and we're compressing them down to a 32 dimensional representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction\n",
    "This section may be ill-named, but the idea here is to see what we can do with our learned model to understand what the learned weights are doing to the input vectors at the first layer. \n",
    "\n",
    "This can give us some insight into what the learned network is doing to the input images to \"best\" represent them so that the 32-dimensional code layer is able to efficiently reconstruct the outputs accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we focus on the weights connecting the first layer (input image that's flattened from 28x28 to 784) to the next layer (128 nodes), then we have a 784 by 128 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = autoencoder.get_weights()[0].T\n",
    "\n",
    "n = 10  # Just plotting the first 10 hidden layer nodes... not all 128\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(weights[i+0].reshape(28, 28))  # Get the weights going to hidden layer node i. Reshape.\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing obvious jumps out, but there appears to be structure in some of these. I think I'd be better off pursuing some sort of interpretability approach (e.g. LIME, etc.). Going to leave this for now and revisit in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sparse Autoencoders\n",
    "Okay. This is really cool. We're going to keep our 32 dimensional coded layer (i.e. \"compress\" our image down from 784 dimensions to 32) BUT we're going to modify the construction to favour learning weights near 0. \n",
    "\n",
    "Why would we want to do that? There's two reasons that are clear to me:\n",
    "1. Our model is under-specified. We have a lot of variables and the objective/loss function combined with our optimizer are not guaranteed to find the same (or even similar) optimized weights every time if we did a different split of train/test OR used a different random seed to initialize things.\n",
    "2. This can increase the variance in the coded representation. Having higher variance often means the coded feature vectors are \n",
    "\n",
    "Note: There can also be reasons why you DON'T want to have sparser features: \n",
    "- Generalization (predicting on unseen data) can be worse. \n",
    "- Robustness can be worse (see https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%203%20-%20Autoencoders.ipynb for a great approach that introduces noise in the inputs to address this). \n",
    "\n",
    "But, for this section, we're going to assume we want to have a sparser representation learned. And for simplicity, we're going to use a shallower network. Just\n",
    "- Input Layer\n",
    "- Code Layer\n",
    "- Output Layer\n",
    "\n",
    "To interpret what we get, we'll compare a vanilla learned network (just like we did above, but with less layers) against a network learned with a penalty for having non-zero weights. \n",
    "\n",
    "Here's an example of using a regularizer. With Keras, this is super easy. Just add an additional argument to the specification of the Code Layer to indicate that the weights have a \"constraint\" on them:\n",
    "```\n",
    "code = Dense(code_size, activation='relu', activity_regularizer=l1(10e-6))(input_img)\n",
    "```\n",
    "The specific choice of `10e-6` isn't terribly clear to me. But, according to https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798 it's usually a value between `0.001` and `0.000001`. I suspect choosing this is like choosing your learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "code_size = 32\n",
    "\n",
    "input_img = Input(shape=(input_size,))\n",
    "code = Dense(code_size, activation='relu')(input_img)\n",
    "output_img = Dense(input_size, activation='sigmoid')(code)\n",
    "\n",
    "autoencoder_standard = Model(input_img, output_img)\n",
    "autoencoder_standard.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history_standard = autoencoder_standard.fit(x_train, x_train, epochs=20)\n",
    "\n",
    "encoded_standard = Model(input_img, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "code_size = 32\n",
    "\n",
    "input_img = Input(shape=(input_size,))\n",
    "code = Dense(code_size, activation='relu', activity_regularizer=l1(10e-6))(input_img)  # Note the regularizer\n",
    "output_img = Dense(input_size, activation='sigmoid')(code)\n",
    "\n",
    "autoencoder_regularized = Model(input_img, output_img)\n",
    "autoencoder_regularized.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history_regularized = autoencoder_regularized.fit(x_train, x_train, epochs=20)\n",
    "\n",
    "encoded_regularized = Model(input_img, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regularized model reconstruction\n",
    "plot_autoencoder_outputs(autoencoder_regularized, (28, 28), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really good still! And we can compare the lsos functions of vanilla and regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the loss functions. Note that since this is an auto-encoder, we only have the loss variable to plot. No accuracy.\n",
    "plot_compare_histories([history_standard, history_regularized], \n",
    "                       ['Standard Autoencoder', 'Regularized Autoencoder'], plot_accuracy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Model Structure\n",
    "How can we understand or verify that the regularized model is indeed more sparse? \n",
    "\n",
    "To start, we can have the models predict the \"coded\" layer on the held out test set `x_test`. This gives a 32-dimensional vector for each image in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte the mean of each 32 dimensional predicted \"code\" across the test set\n",
    "encoded_standard.predict(x_test).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the regularized model.\n",
    "encoded_regularized.predict(x_test).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terribly easy to interpret the above with the output truncated. We can take the mean() across the 32 dimensions too, to get a single number that communicates the average value of the 32 dimensional coded representations of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_standard.predict(x_test).mean())\n",
    "print(encoded_regularized.predict(x_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a big difference between standard and regularized, but there is something. However, that could be due to different training runs... the weights that are learned can vary a lot with different train/test splits and different random initializations.\n",
    "\n",
    "Also, each element is `>=0`. I don't entirely understand why there are no negative values. Maybe something to do with the loss function and inputs only being non-negative? Something to investigate further in the future.\n",
    "\n",
    "Lastly, to better compare the learned coded representations, let's plot the distribution of the coded vectors for the test set. This is a more meaningful visualization of the \"coded\" layer for the standard VS regularized models (compared to just reporting the means as we did above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scores = encoded_standard.predict(x_test).ravel()  # ravel() flattens the matrix of 1000 by 32\n",
    "regularized_scores = encoded_regularized.predict(x_test).ravel()\n",
    "sns.distplot(standard_scores, hist=False, label='standard model')\n",
    "ax = sns.distplot(regularized_scores, hist=False, label='regularized model')\n",
    "ax.set(xlabel='Coded Vector Value', ylabel='Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see that the regularized model has a higher density at lower values than the standard/vanilla one. That aligns with our expectations that the regularized model has more values at/near 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Weights\n",
    "Last thing I'd like to do is compare the weights visualization of the standard vs regularized. \n",
    "\n",
    "We've got a 32 node coded layer so going to plot them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = encoded_standard.get_weights()[0].T\n",
    "\n",
    "n = 32  # Specify how many of the hidden layer nodes to plot\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(4, n/4, i + 1)\n",
    "    plt.imshow(weights[i+0].reshape(28, 28))  # Get the weights going to hidden layer node i. Reshape.\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = encoded_regularized.get_weights()[0].T\n",
    "\n",
    "n = 32  # Specify how many of the hidden layer nodes to plot\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(4, n/4, i + 1)\n",
    "    plt.imshow(weights[i+0].reshape(28, 28))  # Get the weights going to hidden layer node i. Reshape.\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing really stands out. Some of them seem to indicate structure, like a ring, but both regularized and standard show it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
