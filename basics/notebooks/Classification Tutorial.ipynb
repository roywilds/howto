{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Created: 2019-09-22\n",
    "Author: Roy Wilds\n",
    "\n",
    "Updates\n",
    "2019-10-02: Added RF classifier\n",
    "2019-11-17: Cleaned up for push to github\n",
    "```\n",
    "\n",
    "# About this notebook\n",
    "This notebook captures the typical steps involved in building a classifier using pandas and sklearn.\n",
    "\n",
    "It includes some data manipulation to create the classes to be used (the chosen dataset didn't have explicit labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "This uses the amazon fire CSV file from Kaggle: https://www.kaggle.com/gustavomodelli/forest-fires-in-brazil\n",
    "\n",
    "It's a nice dataset that has timestamps, categorical, and numerical features. Not overly complicated, but a nice starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = '~/data/amazon.csv'\n",
    "df = pd.read_csv(csvfile, quotechar='\"', encoding = \"ISO-8859-1\") #, parse_dates=[4]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the presence of the correct encoding argument. Initial attempt to load the data file failed with a Unicode error (the data is from Brazil).\n",
    "\n",
    "Running a file command points us to the correct encoding:\n",
    "```\n",
    "$ file data/amazon.csv \n",
    "data/amazon.csv: ISO-8859 text, with CRLF line terminators\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's properly dtype the various columns, and going to provide the option to translate the \"month\" to English.\n",
    "Note we could have handled the date column during the `read_csv()` step by adding the `parse_dates` arg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['state'] = df['state'].astype('category')\n",
    "df['month'] = df['month'].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugese_months = list(df['month'].cat.categories)\n",
    "portugese_months.sort()\n",
    "# We sort so that the explicit ordering of english months here is correct!\n",
    "english_months = ['April','August','December','February','January','July','June','May','March','November','October','September']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_months = dict(zip(portugese_months,english_months))\n",
    "translate_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'].replace(translate_months, inplace=True)\n",
    "df['month'] = df['month'].astype('category')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classes\n",
    "You may have noticed that we don't actually have any obvious labels! \n",
    "\n",
    "We could try predicting some of the categorical variables... For example, maybe you can predict the month based on the other columns (ignoring the `date` feature obviously).\n",
    "\n",
    "But, here I'm going to be simple with a 2-class problem: \"Lots of Fires\" (`high`) vs \"Fewer Fires\" (`low`). This will be determine by whether or not the number is more than 1 standard-deviation1 away from the mean for the particular `state, month` combination in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's probably a pandas way to do this cleverly using groupby and agg() \n",
    "# but I can't figure out all the reshaping required.\n",
    "states = list(df['state'].cat.categories)\n",
    "months = list(df['month'].cat.categories)\n",
    "import numpy as np\n",
    "df['class'] = 'low' # Start with everything 'low'\n",
    "nstd = 1 # Number of standard deviations to be considered 'high'\n",
    "for s in states:\n",
    "    for m in months:\n",
    "        mu = df[(df['state'] == s ) & (df['month'] == m)]['number'].mean()\n",
    "        sigma = df[(df['state'] == s ) & (df['month'] == m)]['number'].std()\n",
    "        # Wasn't able to get this working using pandas/groupby/etc. ops. Had to resort to a loop.\n",
    "        # At least it's linear in the dataframe size.\n",
    "        for index, row in df[(df['state'] == s ) & (df['month'] == m)].iterrows():\n",
    "            if row['number'] > mu+nstd*sigma:\n",
    "                df.iloc[index,5] = 'high' # THIS IS BRITTLE. Relies on specific shape for \"df\".\n",
    "                \n",
    "        # #Failed attempts to do this more pythonically\n",
    "        # print( (df['state'] == s ) & (df['month'] == m) & ( df['number'] > 0).describe() )\n",
    "        # df['class'] = np.where((df['state'] == s ) & (df['month'] == m) & ( (abs(df['number']-mu)/sigma)>0.01),'high','low')\n",
    "        # df[(df['state'] == s ) & (df['month'] == m) & (abs(df['number']-mu)/sigma>0)]['class'] = 'high'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "Always good to understand the raw data before jumping into modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the number of entries per state\n",
    "df.groupby(['state'])['number'].agg('count').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the total number of fires per state\n",
    "df.groupby(['state'])['number'].agg('sum').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the total number of fires per state, colouring the numbers that were determined to be class=\"high\"\n",
    "#Not a terribly informational plot, but useful plotting technique in general.\n",
    "df.groupby(['state','class'])['number'].agg('sum').unstack().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Going to build a model to predict the `class` from the `state` and `number` features.\n",
    "\n",
    "Need to convert the `state` categorical feature into features that can be consumed by LR or RF.\n",
    "An ordinal encoding doesn't make sense (there's no simple ordering of the states... maybe by latitude since that could be a sensible ordering for climate/weather, but skipping that for now).\n",
    "\n",
    "Will use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest to make a copy and then deal with the one-hot encoding for the 'state' categorical columns.\n",
    "lrdf = df.copy()\n",
    "lrdf = pd.concat([df,pd.get_dummies(df['state'], prefix='state')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns we don't need.\n",
    "lrdf.drop(['year'], axis=1, inplace=True)\n",
    "lrdf.drop(['state'], axis=1, inplace=True)\n",
    "lrdf.drop(['date'], axis=1, inplace=True)\n",
    "lrdf.drop(['month'], axis=1, inplace=True)\n",
    "lrdf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lrdf\n",
    "labels = lrdf['class']\n",
    "data.drop(['class'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Make train/test sets with a 30% test size.\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = logreg.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = confusion_matrix(labels_test, pred_test)\n",
    "# Assume that the class=='high' is the Positive Case (i.e. what we care about classifying)\n",
    "tp, fn, fp, tn = m.ravel()\n",
    "print(m)\n",
    "#print('tn = {}, fp = {}, fn = {}, tp = {}'.format(tn,fp,fn,tp))\n",
    "print('Using class=\"high\" as the positive prediction (i.e. a true prediction).')\n",
    "precision = tp/(tp+fp+0.)\n",
    "recall = tp/(tp+fn+0.)\n",
    "print('Precision = {:.2f} and Recall = {:.2f}'.format(precision,recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Threshold\n",
    "Rather than using the default 0.5 threshold for determining if a prediction is `high` or not, we can vary a threshold from 0 to 1 to control the precision/recall tradeoff of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0.1,0.9,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_probs = logreg.predict_proba(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So 1st col is probability of class='high' and 2nd col is probability of class='low'\n",
    "pred_test_probs[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = [], []\n",
    "for theta in thetas:\n",
    "    pred_test = np.where(pred_test_probs[:,0] >= theta, 'high','low')\n",
    "    m = confusion_matrix(labels_test, pred_test)\n",
    "    # Assume that the class=='high' is the Positive Case (i.e. what we care about classifying)\n",
    "    tp, fn, fp, tn = m.ravel()\n",
    "    precision.append(tp/(tp+fp+0.))\n",
    "    recall.append(tp/(tp+fn+0.))\n",
    "logreg_thetas = pd.DataFrame()\n",
    "logreg_thetas['threshold']=thetas\n",
    "logreg_thetas['precision'] = precision\n",
    "logreg_thetas['recall'] = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_thetas.plot(x='threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot is typical of the recall/threshold tradeoff. You get better precision (i.e. fewer mistakes) at the cost of missing more of the true (i.e. high) predictions (lower recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(data_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = rf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = confusion_matrix(labels_test, pred_test)\n",
    "# Assume that the class=='high' is the Positive Case (i.e. what we care about classifying)\n",
    "tp, fn, fp, tn = m.ravel()\n",
    "print(m)\n",
    "#print('tn = {}, fp = {}, fn = {}, tp = {}'.format(tn,fp,fn,tp))\n",
    "print('Using class=\"high\" as the positive prediction (i.e. a true prediction).')\n",
    "precision = tp/(tp+fp+0.)\n",
    "recall = tp/(tp+fn+0.)\n",
    "accuracy = accuracy_score(labels_test, pred_test)\n",
    "print('Precision = {:.2f} and Recall = {:.2f}'.format(precision,recall))\n",
    "print('Accuracy = {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a great example of why Accuracy isn't a good metric when there's class imbalance.\n",
    "In our case, we've got roughly a 10 to 1 class imbalance and the model gets the class='low' right lots, but for the class='high' case we're not doing great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0.1,0.9,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_probs = rf.predict_proba(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not 'high', 'low' then ensure you change [:,0] to the correct column slice to use!\n",
    "precision, recall = [], []\n",
    "for theta in thetas:\n",
    "    pred_test = np.where(pred_test_probs[:,0] >= theta, 'high','low')\n",
    "    m = confusion_matrix(labels_test, pred_test)\n",
    "    # Assume that the class=='high' is the Positive Case (i.e. what we care about classifying)\n",
    "    tp, fn, fp, tn = m.ravel()\n",
    "    precision.append(tp/(tp+fp+0.))\n",
    "    recall.append(tp/(tp+fn+0.))\n",
    "rf_thetas = pd.DataFrame()\n",
    "rf_thetas['threshold']=thetas\n",
    "rf_thetas['precision'] = precision\n",
    "rf_thetas['recall'] = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_thetas.plot(x='threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat RF but with k-fold Cross Validation\n",
    "Thus far have been using the test/train split with 33% for test. \n",
    "This section is to do k-fold Cross Validation in order to get an estimate on the errors for the model accuracy.\n",
    "\n",
    "Also an opportunity to have some error bars on our precision/recall plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD just provides indexes, so we can just do it on the data (not labels) since they're the same size and share the same indices\n",
    "nfolds = 5\n",
    "kf = KFold(n_splits=nfolds)\n",
    "kf.get_n_splits(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to loop thru the KFOLDS and also through the different thresholds.\n",
    "# Yields a NTHRESHOLD rows x KFOLDS cols\n",
    "nthresholds = 21\n",
    "thetas = np.linspace(0.1, 0.9, nthresholds)\n",
    "precision, recall = np.zeros(shape=(nthresholds, nfolds)), np.zeros(shape=(nthresholds, nfolds))\n",
    "\n",
    "ifold = 0\n",
    "for train_index, test_index in kf.split(data):\n",
    "    data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "    labels_train, labels_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    rf.fit(data_train,labels_train)\n",
    "    pred_test_probs = rf.predict_proba(data_test)\n",
    "    \n",
    "    itheta = 0\n",
    "    for theta in thetas:\n",
    "        pred_test = np.where(pred_test_probs[:,0] >= theta, 'high','low')\n",
    "        m = confusion_matrix(labels_test, pred_test)\n",
    "        # Assume that the class=='high' is the Positive Case (i.e. what we care about classifying)\n",
    "        tp, fn, fp, tn = m.ravel()\n",
    "        precision[itheta, ifold] = tp/(tp+fp+0.)\n",
    "        recall[itheta, ifold] = tp/(tp+fn+0.)\n",
    "        itheta += 1\n",
    "    \n",
    "    ifold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_errors = np.std(precision, axis=1)\n",
    "precision_line = np.mean(precision, axis=1)\n",
    "\n",
    "recall_errors = np.std(recall, axis=1)\n",
    "recall_line = np.mean(recall, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Precision and Recall - 1 Std Dev shown')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.errorbar(thetas, recall_line, yerr=recall_errors, c='red', capsize=3)\n",
    "plt.errorbar(thetas, precision_line, yerr=precision_errors, c='blue', capsize=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
